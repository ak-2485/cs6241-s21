{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for 2021-02-16\n",
    "\n",
    "In these exercises, we will look at regularization for least squares in the context of a curve fitting problem.  Before we begin, it may be helpful to know a little about the [Legendre polynomails](https://en.wikipedia.org/wiki/Legendre_polynomials), which play an important role in approximation\n",
    "theory and in the development of Gaussian quadrature.  Legendre polynomials may be computed by the recurrence\n",
    "$$\n",
    "  (n+1) P_{n+1}(x) = (2n+1)x P_n(x) - n P_{n-1}(x)\n",
    "$$\n",
    "where $P_0(x) = 1$ and $P_1(x) = x$.  These polynomials are orthogonal with respect to the $L^2$ inner product\n",
    "on $[-1, 1]$:\n",
    "$$\n",
    "  \\langle P_m, P_n \\rangle_{L_2} = \\int_{-1}^1 P_m(x) P_n(x) \\, dx = \\frac{2}{2n+1} \\delta_{mn}.\n",
    "$$\n",
    "The normalized Legendre polynomials are\n",
    "$$\n",
    "  Q_n(x) = \\sqrt{\\frac{2n+1}{2}} P_n(x),\n",
    "$$\n",
    "and they form an orthonormal basis for $L^2$ on $[-1,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function legendre(xx, dmax)\n",
    "    P = zeros(length(xx), dmax+1)\n",
    "    P[:,1] .= 1.0\n",
    "    if dmax > 0\n",
    "        P[:,2] .= xx\n",
    "    end\n",
    "    for n = 1:dmax-1\n",
    "        P[:,n+2] .= ( (2*n+1) .* xx .* P[:,n+1] - n * P[:,n] )/(n+1)\n",
    "    end\n",
    "    return P\n",
    "end\n",
    "\n",
    "function nlegendre(xx, dmax)\n",
    "    Q = legendre(xx, dmax)\n",
    "    for n = 0:dmax\n",
    "        Q[:,n+1] .*= sqrt(n+0.5)\n",
    "    end\n",
    "    return Q\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = range(-1, 1, length=100)\n",
    "plot(xx, nlegendre(xx, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO*: Suppose $g(x) = \\sum_{j=0}^d c_j Q_j(x)$, where the $Q_j$ are the normalized Legendre polynomials described above.  Then $\\|g\\|^2_{L^2} \\equiv \\int_{-1}^1 g(x)^2 \\, dx$ and $\\|c\\|^2 = \\sum_{j=0}^d c_j^2$ are the same.  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the problem of approximating a simple bump function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = exp(-20.0*x^2)\n",
    "plot(xx, f.(xx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that approximating this function by interpolation on an equi-spaced grid gives bad results near the end of the interval due to [Runge's phenomenon](https://en.wikipedia.org/wiki/Runge%27s_phenomenon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmax = 20\n",
    "xs = range(-1.0, 1.0, length=dmax+1)\n",
    "A = nlegendre(xs, dmax)\n",
    "b = f.(xs)\n",
    "c = A\\b\n",
    "print(\"Max error: $(norm(f.(xx).-nlegendre(xx, dmax)*c, Inf))\")\n",
    "plot(xx, f.(xx))\n",
    "plot!(xx, nlegendre(xx, dmax)*c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This turns out to be an *ill-conditioned* problem, but increasing the number of data points helps a great deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = range(3, 60, step=1)\n",
    "nlegendreA(n, dmax) = nlegendre(range(-1.0, 1.0, length=n), dmax)\n",
    "plot(ns, cond.(nlegendreA.(ns, dmax)), yscale=:log10, xlabel=\"Number of points\", ylabel=\"Conditioning of LS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmax = 20\n",
    "xs = range(-1.0, 1.0, length=201)\n",
    "A = nlegendre(xs, dmax)\n",
    "b = f.(xs)\n",
    "c_hi = A\\b\n",
    "print(\"Max error: $(norm(f.(xx).-nlegendre(xx, dmax)*c_hi, Inf))\")\n",
    "plot(xx, f.(xx))\n",
    "plot!(xx, nlegendre(xx, dmax)*c_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Now, suppose that we have data at 21 points for a degree 20 polynomial, and we aren't able to get more data.  Let's consider two different forms of regularization: adaptively choosing a polynomial degree, or Tikhonov regularization.  In each case, we use a [cross-validation statistic (LOOCV)](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) (also known as the [PRESS statistic](https://en.wikipedia.org/wiki/PRESS_statistic)) to judge our fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the polynomial degree\n",
    "\n",
    "If we were able to evaluate the error on a dense grid, a natural way to choose the polynomial degree for fitting to 21 points would be to choose it to minimize the error.  In this problem, since we have a ground truth, we can do this; in general, though, we don't like methods that require us to use much more data for evaluation than for fitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmax = 20\n",
    "npts = dmax+1\n",
    "xs = range(-1.0, 1.0, length=npts)\n",
    "#xs = 2.0*rand(dmax+1).-1.0\n",
    "A = nlegendre(xs, dmax)\n",
    "b = f.(xs)\n",
    "Q, R = qr(A)\n",
    "QTb = Q'*b\n",
    "\n",
    "Ahi = nlegendre(xx, dmax)\n",
    "err(k) = norm(f.(xx) - Ahi[:,1:k]*(R[1:k,1:k]\\QTb[1:k]), Inf)\n",
    "ks = range(1, dmax, step=1)\n",
    "scatter(ks, err.(ks), yscale=:log10, xlabel=\"Degree\", ylabel=\"Max error on mesh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of leave-one-out cross validation (LOOCV) is that we check each data point on a model trained without\n",
    "that data point.  That is\n",
    "$$\n",
    "  \\mbox{LOOCV} = \\frac{1}{n} \\sum_{i=1}^n (b_i - b_{[-i]})^2\n",
    "$$\n",
    "where $b_{[-i]}$ is the prediction at point $i$ based on all the data except that at point $i$.\n",
    "\n",
    "We start by doing this the slow way (which is not so slow for this small a problem, admittedly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_loocv = zeros(dmax)\n",
    "for k = 1:dmax\n",
    "    for i = 1:npts\n",
    "        mask = BitArray(i != j for j = 1:npts)\n",
    "        cik = A[mask, 1:k]\\b[mask]\n",
    "        slow_loocv[k] += (b[i] - dot(A[i,1:k], cik))^2/npts\n",
    "    end\n",
    "end\n",
    "scatter(ks, slow_loocv, yscale=:log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For least squares, there is a clever linear algebra trick that allows us to compute the LOOCV statistic without actually computing the leave-one-out fit for each of the data points from scratch.  The key is to compute the diagonal of the \"influence matrix\" $H$ that maps the data points to the vector of model evaluations at the data points (these values are sometimes called the [leverage scores](https://en.wikipedia.org/wiki/Leverage_(statistics)) of the data points, and play an important role in outlier detection as well).  In terms of this matrix, we have\n",
    "$$\n",
    "  \\mbox{LOOCV} = \\frac{1}{n} \\sum_{j=1}^n \\left( \\frac{r_j}{1-h_{jj}} \\right)^2\n",
    "$$\n",
    "where $r = (I-H)b$ is the residual vector and $h_{jj}$ is the $j$th diagonal entry of $H$.\n",
    "\n",
    "If we use the economy QR factorization $A=QR$ for a rectangular $A$, the influence matrix is $H = QQ^T$, the least squares projection onto the range space of $A$.  Thus, the economy QR factorization gives us not only a fast way to compute the least squares solution, but also a fast way to compute the LOOCV statistic.\n",
    "\n",
    "*TODO* (2): Fill in the following code to compute the LOOCV using the fast algorithm.  You should be able to do this by updating the residual vector $r$ and diagonal vector $h$ in $O(n)$ time per step of the main loop (forming $Q$ expicitly is the most expensive piece of this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Array(Q)\n",
    "loocv = zeros(dmax)\n",
    "h = zeros(npts)\n",
    "r = b\n",
    "for k = 1:dmax\n",
    "    # TODO: Update r\n",
    "    # TODO: Update h\n",
    "    loocv[k] = sum( (r./(1.0.-h)).^2 )/npts\n",
    "end\n",
    "\n",
    "print(\"Max relerr: $(norm((slow_loocv-loocv)./slow_loocv, Inf))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a Tikhonov parameter\n",
    "\n",
    "In the case of Tikhonov regularization, we again start by cheating and taking advantage of the fact that we know the \"ground truth.\"  For small problems, the fastest way to do this is via the SVD.\n",
    "\n",
    "*TODO*: Explain the `tikhonov_fit` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nlegendre(xs, dmax)\n",
    "UA, SA, VA = svd(A)\n",
    "tikhonov_fit(b, λ) = VA*((UA'*b).*(SA./(SA.^2 .+ λ^2)))\n",
    "\n",
    "Ahi = nlegendre(xx, dmax)\n",
    "\n",
    "err_tik(λ) = norm(f.(xx) - Ahi*tikhonov_fit(b, λ), Inf)\n",
    "λs = exp10.(range(-6, 3, step=0.2))\n",
    "scatter(λs, err_tik.(λs), xscale=:log10, yscale=:log10, xlabel=\"Lambda\", ylabel=\"Max error on mesh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO*: Explain the code below, which computes the LOOCV statistic for the regularized least squares problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loocv = zeros(length(λs))\n",
    "UTb = UA'*b\n",
    "h = zeros(npts)\n",
    "for k = 1:length(λs)\n",
    "    λ = λs[k]\n",
    "    HS = SA.^2 ./ (SA.^2 .+ λ^2)\n",
    "    r = b - UA*(HS.*UTb)\n",
    "    h = [sum(UA[i,:].^2 .* HS) for i in 1:npts]\n",
    "    loocv[k] = sum(( r ./ (1.0.-h) ).^2)/npts\n",
    "end\n",
    "scatter(λs, loocv, xscale=:log10, yscale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
