{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for 2021-03-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative matrix factorization\n",
    "\n",
    "Consider the problem of finding the best rank 3 NMF approximation of the following 10-by-5 data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [0 0 0 1 0;\n",
    "     0 0 0 0 1;\n",
    "     0 0 0 0 1;\n",
    "     1 0 1 0 0;\n",
    "     1 0 0 0 0;\n",
    "     0 1 0 0 0;\n",
    "     1 0 1 1 0;\n",
    "     0 1 1 0 0;\n",
    "     0 0 1 1 1;\n",
    "     0 1 1 0 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the methods we have for NMF are iterative (unless some separability condition is satisfied), which means we need an initial guess.\n",
    "We use an SVD-based initial guess due to Boutsidis and Gallapoulos ([2007 paper](http://scgroup.hpclab.ceid.upatras.gr/faculty/stratis/Papers/HPCLAB020107.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function svd_nmf_init(A, k)\n",
    "    U, σs, V = svd(A)\n",
    "    W = zeros(size(A)[1], k)\n",
    "    H = zeros(k, size(A)[2])\n",
    "    for j = 1:k\n",
    "        xp = max.( U[:,j], 0.0)\n",
    "        xn = max.(-U[:,j], 0.0)\n",
    "        yp = max.( V[:,j], 0.0)\n",
    "        yn = max.(-V[:,j], 0.0)\n",
    "        norm_xp = norm(xp)\n",
    "        norm_xn = norm(xn)\n",
    "        norm_yp = norm(yp)\n",
    "        norm_yn = norm(yn)\n",
    "        norms_p = norm_xp * norm_yp\n",
    "        norms_n = norm_xn * norm_yn\n",
    "        if norms_p > norms_n\n",
    "            scale = sqrt(σs[j] * norms_p)\n",
    "            W[:,j] = (scale/norm_xp) * xp\n",
    "            H[j,:] = (scale/norm_xp) * yp\n",
    "        else\n",
    "            scale = sqrt(σs[j] * norms_n)\n",
    "            W[:,j] = (scale/norm_xn) * xn\n",
    "            H[j,:] = (scale/norm_yn) * yn\n",
    "        end\n",
    "    end\n",
    "    return W, H\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projective gradient\n",
    "\n",
    "We will start with the simple projective gradient iteration from the class notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "α = 0.1\n",
    "W, H = svd_nmf_init(A, 3)\n",
    "resids = zeros(500)\n",
    "for k = 1:500\n",
    "    R = A-W*H\n",
    "    resids[k] = norm(R)\n",
    "    Wnew = max.(W + α*R*H', 0.0)\n",
    "    Hnew = max.(H + α*W'*R, 0.0)\n",
    "    W, H = Wnew, Hnew\n",
    "end\n",
    "plot(resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resids[end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lee and Seung\n",
    "\n",
    "The Lee and Seung iteration takes a while to converge, as we can see from a plot of the residual norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, H = svd_nmf_init(A, 3)\n",
    "resids = zeros(500)\n",
    "for k = 1:500\n",
    "    resids[k] = norm(A-W*H)\n",
    "    Wnew = W ./ (W*H*H') .* (A*H')\n",
    "    Hnew = H ./ (W'*W*H) .* (W'*A)\n",
    "    W, H = Wnew, Hnew\n",
    "end\n",
    "plot(resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A-W*H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resids[end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HALS/RRI (3 points)\n",
    "\n",
    "Your job is to implement the HALS iteration (also known as RRI), as described in the class notes.  As with the other iterations, show a plot of the residual, and print the final $W$ and $H$ factors.  Do they look similar to the factors computed by either of the previous methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternating least squares\n",
    "\n",
    "#### The NNLS building block\n",
    "\n",
    "The alternating least squares approach for non-negative matrix factorization requires solving a series of non-negative least squares problem.\n",
    "For this, it's useful to have a reasonable non-negative least squares problem.  We include below an active set solver for minimizing $\\|Ax-b\\|^2$ subject to elementwise non-negativity constraints on $A$, essentially identical to the method proposed by [Bro and De Jong](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291099-128X%28199709/10%2911%3A5%3C393%3A%3AAID-CEM483%3E3.0.CO%3B2-L).\n",
    "\n",
    "This could be made faster by being a little smarter about the linear algebra at each step, and could potentially be a little more stable by not explicitly forming $A^T A$ and $A^T b$ directly (and instead using the QR factorization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function nnls(AtA, Atb, x = [])\n",
    "\n",
    "  # Set up LS problem\n",
    "  (m,n) = size(AtA)\n",
    "\n",
    "  # Set up starting point and tolerance\n",
    "  if isempty(x)\n",
    "    x = AtA\\Atb\n",
    "  end\n",
    "  x = max.(x, 0.0)\n",
    "  tol = 100*eps(eltype(Atb))\n",
    "  maxiter = 1000\n",
    "\n",
    "  # Allocate storage for step, residual, gradient/multipliers\n",
    "  p = zeros(n)\n",
    "  r = zeros(n)\n",
    "  rr = zeros(n)\n",
    "\n",
    "  # Set initial passive set\n",
    "  P = vec(x .> 0.0)\n",
    "\n",
    "  # Main loop\n",
    "  for i = 1:maxiter\n",
    "\n",
    "    # Constrained Newton direction\n",
    "    fill!(p,0.)\n",
    "    rr[:] = AtA*x-Atb\n",
    "    p[P] = AtA[P,P]\\rr[P]\n",
    "\n",
    "    if norm(p, Inf) < tol\n",
    "\n",
    "      # Find constraint that should not be active (most negative multiplier)\n",
    "      m = 0\n",
    "      rrmin = 0.\n",
    "      for j = 1:n\n",
    "        if !P[j] && rr[j] < rrmin\n",
    "          rrmin = rr[j]\n",
    "          m = j\n",
    "        end\n",
    "      end\n",
    "\n",
    "      # Release constraint or return\n",
    "      if rrmin < 0.\n",
    "        P[m] = true\n",
    "      else\n",
    "        return x\n",
    "      end\n",
    "\n",
    "    else\n",
    "\n",
    "      # Determine step length\n",
    "      alpha = 1.\n",
    "      m = 0\n",
    "      for j = 1:n\n",
    "        if p[j] > 0. && x[j] < alpha*p[j]\n",
    "          m = j\n",
    "          alpha = min(alpha, x[j]/p[j])\n",
    "        end\n",
    "      end\n",
    "      \n",
    "      # Take a Newton step and adjust P\n",
    "      if alpha == 1.\n",
    "        x[:] -= p\n",
    "      else\n",
    "        x[:] -= alpha*p\n",
    "        P[m] = false\n",
    "      end\n",
    "      x[:] = max.(x, 0.)\n",
    "\n",
    "    end\n",
    "\n",
    "  end  \n",
    "  println(\"Did not converge\")\n",
    "  x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Atest = rand(10,3)\n",
    "btest = Atest * [1.0 1.0 -1.0]'\n",
    "xtest = nnls(Atest'*Atest, Atest'*btest)\n",
    "rtest = Atest'*(Atest*xtest-btest)\n",
    "println(\"x = $xtest\")\n",
    "println(\"r = $rtest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The alternating least squares algorithm (3 points)\n",
    "\n",
    "Per our discussion in class, one step of alternating least squares for NMF involves two pieces:\n",
    "\n",
    "1. Assuming $W$ is correct, solve a series of non-negative least squares problems (one per column of $H$ or $A$) to fill in $H$.\n",
    "2. Assuming $H$ is correct, solve a series of non-negative least squares problems (one per row of $W$ or $A$) to fill in $W$.\n",
    "\n",
    "Implement this ALS iteration, and again plot convergence.  Note that using the previous iterate for starting guesses for the columns of $H$ or rows of $W$ will probably speed up the NNLS solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor manipulations and the higher-order SVD (HOSVD)\n",
    "\n",
    "We can represent a dense order 3 tensor in Julia as a simple three-way array.  Let's create a simple example with a known CP decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = rand(10,3)\n",
    "G = rand(10,3)\n",
    "H = rand(10,3)\n",
    "A = [sum(F[i,:].*G[j,:].*H[k,:]) for i=1:10, j=1:10, k=1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we construct the flattenings in each mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = zeros(10,100)\n",
    "A2 = zeros(10,100)\n",
    "A3 = zeros(10,100)\n",
    "\n",
    "for i = 1:10\n",
    "    A1[i,:] = reshape(A[i,:,:], 100)\n",
    "    A2[i,:] = reshape(A[:,i,:], 100)\n",
    "    A3[i,:] = reshape(A[:,:,i], 100)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The building block for the HOSVD is the orthogonal factors from SVDs of the various flattenings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1, S1, V1 = svd(A1)\n",
    "U2, S2, V2 = svd(A2)\n",
    "U3, S3, V3 = svd(A3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sense of the SVD, we need to consider a linear transformation in each mode, i.e. a transformation of the form\n",
    "$$\n",
    "  C_{ijk} = \\sum_{p, q, r} A_{pqr} X_{ip} Y_{jq} Z_{kr}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function product3(A, X, Y, Z)\n",
    "    C = zeros(size(X)[1], size(Y)[1], size(Z)[1])\n",
    "    for p = 1:size(X)[2]\n",
    "        for q = 1:size(Y)[2]\n",
    "            for r = 1:size(Z)[2]\n",
    "                C += [A[p,q,r]*X[i,p]*Y[j,q]*Z[k,r] for i=1:size(X)[1], j=1:size(Y)[1], k=1:size(Z)[1]]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return C\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher-order SVD of $A$ now looks like\n",
    "$$\n",
    "  A = S \\times_1 U \\times_2 V \\times_3 W,\n",
    "$$\n",
    "where $U, V, W$ are orthogonal matrices.  In componenent form, this is\n",
    "$$\n",
    "  A_{ijk} = \\sum_{p,q,r} S_{pqr} U_{ip} V_{jq} W_{kr}.\n",
    "$$\n",
    "We can recover $S$ by transposing the orthogonal factors, i.e.\n",
    "$$\n",
    "  S = A \\times_1 U^T \\times_2 V^T \\times_3 W^T,\n",
    "$$\n",
    "or in component form\n",
    "$$\n",
    "  S_{pqr} = \\sum_{i,j,k} A_{ijk} U_{ip} V_{jq} W_{kr}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = product3(A, U1', U2', U3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the $S$ tensor is all zero (save for roundoff) outside of a 3-by-3-by-3 leading subtensor (this is a case where we have *multilinear rank* of $(3,3,3)$).  So we are able to recover a compressed representation of the full tensor in terms of the leading part of the higher-order SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1k = U1[:,1:3]\n",
    "U2k = U2[:,1:3]\n",
    "U3k = U3[:,1:3]\n",
    "Sk = product3(A, U1k', U2k', U3k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(A - product3(Sk, U1k, U2k, U3k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, while truncating the higher-order SVD recovers the tensor exactly in this case, in general we do not have the analogue of the Eckart-Young theorem: truncating the higher-order SVD does *not* generally give the best Frobenius norm approximation of a given multilinear rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating least squares for Tucker\n",
    "\n",
    "The truncated HOSVD does not give the best approximation with a given multilinear rank.  Finding the best approximation is the *Tucker* approximation problem.  It turns out that we can get rid of the core tensor $S$ in the problem\n",
    "$$\n",
    "  \\mbox{minimize } \\|A-S \\times_1 U \\times_2 V \\times_3 W\\|_F^2\n",
    "$$\n",
    "where $U, V, W$ are rectangular matrices with orthonormal columns; the problem is equivalent to\n",
    "$$\n",
    "  \\mbox{maximize } \\|S\\|_F^2 \\mbox{ where } S = A \\times_1 U^T \\times_2 V^T \\times_3 W^T.\n",
    "$$\n",
    "The *alternating least squares* algorithm for the Tucker decomposition updates $U$ assuming that $V$ and $W$ are correct, then updates $V$, then updates $W$.  In each case, we can compute the updated matrix solving the maximization problem by computing an SVD of a flattening; that is, if we let $g_k(B)$ refer to the first $k$ columns of the $U$ matrix in the SVD of $B$, we have\n",
    "$$\\begin{align*}\n",
    "  U &:= g_k( (A \\times_1 I \\times_2 V^T \\times_3 W^T)_{(1)}) \\\\\n",
    "  V &:= g_k( (A \\times_1 U^T \\times_2 I \\times_3 W^T)_{(2)}) \\\\\n",
    "  W &:= g_k( (A \\times_2 U^T \\times_2 V^T \\times_3 I)_{(3)})\n",
    "\\end{align*}$$\n",
    "where the parenthesized subscript indicates the modes that are the columns in the flattening.\n",
    "The \"modal products, flatten, then SVD\" updates can also be rewritten as \"flatten, multiply by a Kronecker product, then SVD.\"  Needless to say, it always takes me some mumbling over indices to get this right.\n",
    "\n",
    "Let's use this approach to demonstrate for the case of a Tucker decomposition of multilinear rank $(2,2,2)$.  The norm of the truncated singular value matrix at each step should be monotonically increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with the orthogonal factors from the HOSVD\n",
    "Ut1 = U1[:,1:2]\n",
    "Ut2 = U2[:,1:2]\n",
    "Ut3 = U3[:,1:2]\n",
    "\n",
    "# Compute an updated matrix factor\n",
    "function update_U(B, k)\n",
    "    UU, SS, VV = svd(B)\n",
    "    return UU[:,1:k], norm(SS[1:k])\n",
    "end\n",
    "\n",
    "# Track the norm of the S factor over iterations\n",
    "Snorm = zeros(10)\n",
    "Snorm[1] = norm(S[1:2,1:2,1:2])\n",
    "for k = 2:10\n",
    "    Ut1, ϕ = update_U(A1 * kron(Ut3, Ut2), 2)\n",
    "    Ut2, ϕ = update_U(A2 * kron(Ut3, Ut1), 2)\n",
    "    Ut3, ϕ = update_U(A3 * kron(Ut2, Ut1), 2)\n",
    "    Snorm[k] = ϕ\n",
    "end\n",
    "\n",
    "# Plot the norm of the S matrix\n",
    "plot(Snorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the differences between $S$ norms at consecutive steps gives us a hint of fast linear convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(Snorm[2:end]-Snorm[1:end-1], yscale=:log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating least squares for CP\n",
    "\n",
    "The CP decomposition is a sum-of-rank-one type of decomposition that does not insist on orthonormal factors, i.e. for the third-order case we have\n",
    "$$\n",
    "  A = \\sum_{j=1}^r \\lambda_j F_{:,j} \\circ G_{:,j} \\circ H_{:,j}.\n",
    "$$\n",
    "Here, too, the standard approach involves alternating least squares problems, followed by normalization of the columns:\n",
    "$$\\begin{align*}\n",
    "  \\mbox{minimize } & \\|A_{(1)} - \\tilde{F} (H \\odot G)^T\\|^2, & \\lambda_{j} F_{:,j} & = \\tilde{F}_{:,j} \\\\\n",
    "  \\mbox{minimize } & \\|A_{(2)} - \\tilde{G} (H \\odot F)^T\\|^2, & \\lambda_{j} G_{:,j} & = \\tilde{G}_{:,j} \\\\\n",
    "  \\mbox{minimize } & \\|A_{(3)} - \\tilde{H} (G \\odot F)^T\\|^2, & \\lambda_{j} H_{:,j} & = \\tilde{H}_{:,j}\n",
    "\\end{align*}$$\n",
    "Recall that $\\odot$ refers to the Khatri-Rao product, which is composed of Kronecker products of matching columns.  This is therefore a very tall skinny least squares problem.  There is some special structure that one can take advantage of, but we are willing to be lazy and formulate it in the usual way.\n",
    "\n",
    "We begin with functions to form the Khatri-Rao product of two matrices, to do column normalization, and to form the tensor represented by a given set of scale factors and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function krprod(B, C)\n",
    "    mB, nB = size(B)\n",
    "    mC, nC = size(C)\n",
    "    BC = zeros(mB*mC, nB)\n",
    "    for j = 1:nB\n",
    "        BC[:,j] = kron(B[:,j], C[:,j])\n",
    "    end\n",
    "    return BC\n",
    "end\n",
    "\n",
    "function colnormalize(Ft)\n",
    "    mF, nF = size(Ft)\n",
    "    λs = zeros(nF)\n",
    "    F = zeros(mF, nF)\n",
    "    for j = 1:nF\n",
    "        λs[j] = norm(Ft[:,j])\n",
    "        F[:,j] = Ft[:,j] / λs[j]\n",
    "    end\n",
    "    return F, λs\n",
    "end\n",
    "\n",
    "function form_cp(λs, F, G, H)\n",
    "    A = zeros(size(F)[1], size(G)[1], size(H)[1])\n",
    "    for p = 1:length(λs)\n",
    "        for i = 1:size(F)[1]\n",
    "            for j = 1:size(G)[1]\n",
    "                for k = 1:size(H)[1]\n",
    "                    A[i,j,k] += λs[p] * F[i,p] * G[j,p] * H[k,p]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return A\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The alternating least squares algorithm (3 points)\n",
    "\n",
    "Complete the following loop for the alternating least squares algorithm for the CP decomposition, and plot the residual error on a semi-log scale as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a good (but not exact) initial guess\n",
    "Fc, λsF = colnormalize(F)\n",
    "Gc, λsG = colnormalize(G)\n",
    "Hc, λsH = colnormalize(H)\n",
    "Fc += 0.1 * rand(10,3)\n",
    "Gc += 0.1 * rand(10,3)\n",
    "Hc += 0.1 * rand(10,3)\n",
    "λs = λsF .* λsG .* λsH\n",
    "\n",
    "resids = zeros(500)\n",
    "resids[1] = norm(A-form_cp(λs, Fc, Gc, Hc))\n",
    "for k = 2:500\n",
    "    # TODO: Fill in with ALS step\n",
    "    resids[k] = norm(A-form_cp(λs, Fc, Gc, Hc))\n",
    "end\n",
    "# TODO: Plot residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Khatri-Rao and Gram matrices (3 points)\n",
    "\n",
    "The Gram matrix that appears in the normal equations for a least squares problem involving a Khatri-Rao product (i.e. minimizing $\\|(B \\odot C) u - d\\|^2$) has a special form:\n",
    "$$\n",
    "  (B \\odot C)^T (B \\odot C) = (B^T B) .* (C^T C)\n",
    "$$\n",
    "where $.*$ refers to the Hadamard (elementwise) product.  Explain why this is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
